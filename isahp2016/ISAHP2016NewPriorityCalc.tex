\documentclass[11pt]{article}
%\documentclass{minimal}
\usepackage[annataritalic]{tengwarscript}
\usepackage{framed}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage[all,cmtip]{xy}
\usepackage{fancyhdr}
\usepackage{titling}
\usepackage{soul}
\usepackage{natbib}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[top=1in,bottom=1in,left=1.5in,right=1.5in]{geometry}
%\usepackage{fullpage}
\title{NEW PRIORITY CALCULATIONS}
\pagestyle{fancy}
\chead{}
\lhead{\textit{ISAHP Article: \thetitle}}
\rhead{}
\lfoot{\textit{International Symposium on the \\ Analytic Hierarchy Process}}
\cfoot{\thepage}
\rfoot{London, U.K. \\ August 4-7, 2016}
\newtheorem{definition}{Definition}
\newtheorem{note}{Note}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}
\renewcommand{\maketitle}{\begin{center}{\Large \textbf{\thetitle}} \end{center}}
%%%Numbered bib section
\renewcommand{\bibsection}{\section{Key References}}
%%%%%%%%%
\begin{document}
\maketitle
\thispagestyle{fancy}

\begin{abstract}
There are many methods of deriving priority vectors from pairwise comparison
matrices, e.g. the standard largest eigenvector, the geometric mean, Harker's
method, the least squares method, etc.  Through our work on the SimpleAHP
web application, we discovered an issue with most of these methods, that
confused our users.  The issue arises when there are two or more voters
on a particular pairwise set, and those users have opposite votes on everything
(we call them doppleganger voters).  Given doppleganger voters one would
expect the resulting priority sets to be inverses, or at least have reversed
rankings.  This is not always the case (this result has long been known, but
the SimpleAHP web application made this idea more apparent).

While the geometric mean method does address this issue, it has its own
shortcomings.  Therefore in this paper we describe two new priority vector
calculations that address doppleganger voters, while retaining the graph
theoretic spirit of the eigenvector method.  We compare and contrast the
results of the priority calculations with some of the standard methods
(eigenvector and geoemtric mean) on some differentiating examples.
In addition we provide open source implementations of the new calculations in
several languages (Python, R, Excel) in a free available github repository.  \end{abstract}

% $$
% \xymatrix{
%  & \hbox{\mbox{\emph{Areas Choice Goal}}} \ar[dl] \ar[d]|{\hbox{Alternatives}} \ar[dr] \ar[drr]\\
% Math & Animals & Library & Kitchen }$$

\section{Introduction}
The process of deriving a priority vector from a pairwise comparison matrix is a
very well studied problem.  However, our work creating the SimpleAHP web 
application (see \cite{adamsmont} for more information) brought to light some
subtleties surrounding the standard largest eigenvector calculation.
In the SimpleAHP web application, participants can provide symbolic votes
of \emph{better} and \emph{much better}, and the facilitator can adjust the
meaning of those symbolic votes.  This structure allows for easy construction
of doppleganger voters.
\begin{definition}[Doppleganger voter pair]
	Two voters on a pairwise comparison set are said to be \ul{dopplergangers} if
	the pairwise comparison matrix of one user is the transpose of the other user.
\end{definition}
Doppleganger voters happened naturally with our participants because of the
reduced amount of choice.  However, a second natural way to construct doppleganger
voters in the SimpleAHP web application is to change the meaning of \emph{better} 
to be a number less than 1, and similarly for \emph{much better}.  Effectively we
are making an inverse interpretation of what \emph{better} and \emph{much better} 
mean.

We constructed a doppleganger voter in this fashion on our first participant
and attempted to verify our calculations by verifying that the doppleganger voter
had reversed the order of the alternatives being compared.  In fact we found
that it did not reverse the order, and eventually we discovered this fact is
indeed well known, (e.g. see \cite{choo2004common}).  However this behavior
of the largest eigenvector was troublesome for our target user, namely youth
and those with no experience in AHP theory.  Because this concept, of
doppleganger voters having reversed priorities, we use the
phrase \ul{the doppleganger property} in the following sense.
\begin{definition}[Doppleganger property]
	We say that a priority vector calculation $\mathcal{P}$
	has the \ul{weak doppleganger property} if, for
	all pairwise comparison matrices $M$ the ranking of $\mathcal{P}(M)$
	is the inverse ranking of $\mathcal{P}(M^{T})$.
	
	We say that a priority vector calculation $\mathcal{P}$ has
	the \ul{strong doppleganger property} if, for all pairwise comparison
	matrices $M$, we have
	$$\frac{1}{\mathcal{P}(M)} = \mathcal{P}(M^T).$$
\end{definition}

In this paper we put forward two novel methods for calculating priority
vectors from a pairwise comparison matrix.  Both are adaptations of the
standard largest eigenvector calculation, and both are the result of a limiting
process very similar to the largest eigenvector's standard computational
algorithm.

\section{Literature Review}
This work is heavily influenced by our work in \cite{adamsmont} where we create
tools and techniques that allow youths, and those inexperienced in AHP, to
nonetheless use AHP successfully on their own, without the need of 
facilitator or expert in the theory.  Through that work we discovered some
properties of the largest eigenvector priority vector calculation that
confused our target audience (and in truth it confused us initially as well).

This behavior of the largest eigenvector, namely that the largest eigenvector
of the transpose and the largest eigenvector of the original pairwise matrix need
not be related, is well known and can be found in \cite{choo2004common} or
\cite{saaty1990eigenvector}.
However, because of the simplicity of changing symbolic votes in the SimpleAHP
web interface, this property of the largest eigenvector is more readily observable
and causes issues for those new to AHP theory.
The geometric average is a standard method that remedies the transpose issue, but
it has its own shortcomings (see \cite{barzilai1994ahp} for details of using
geometric averages of columns).  Instead we opted to create two new
eigenvector inspired, priority vector calculations, both of which respect the
doppleganger property.

\section{Hypotheses/Objectives}
This paper defines two new priority vector calculations, algorithmically.
We compare and contrast these new calculations with the standard eigenvector
method and provide some examples that elucidate the differences in approach.

\section{Research Design/Methodology}
For this research, we develop programming libraries for several languages that
implement the new algorithm.  These libraries are freely available on our github
page at \cite{githubahppri}.  In addition there is a publicly available gitter
room for discussion of these algorithms and results (see \cite{gitterahppri}).

The two new calculations are based upon the standard largest eigenvector calculation
method. In particular we mean the following method for calculating the largest
eigenvector.

\begin{definition}[Largest eigenvector algorithm]
	\label{def:eigen}
	Let $M$ be an $n \times n$ pairwise comparison matrix. The 
	largest eigenvector is the limit of the
	sequence of vectors $\mathbf{v}_k$, where
	\begin{eqnarray*}
		\mathbf{v}_0 &=& (1, 1, \ldots, 1) \\
		\mathbf{v}_{k+1} &=& Normalize(M \times \mathbf{v}_k) \\
		\mathcal{E}(M) &=& \lim_{k \to \infty} \mathbf{v}_k
	\end{eqnarray*}
	and $Normalize(\mathbf{v})$ means divide the vector $\mathbf{v}$ by the sum of its entries.  And by $M \times \mathbf{v}_k$ we mean standard matrix multiplication.
\end{definition}

Our first new priority vector calculation, which we call the \ul{Geometric Eigenvector} 
calculation, tweaks Definition \ref{def:eigen} by changing the notion of ``matrix
multiplication'' to that of geometric average.  We can state the definition as follows.
\begin{definition}[Geometric Eigenvector algorithm]
	\label{def:geigen}
	Let $M$ be an $n \times n$ pairwise comparison matrix.  The 
	\ul{Geometric Eigenvector} is the limit of the following sequence of
	vectors $\mathbf{v}_0, \mathbf{v}_1, \ldots$ where
	\begin{eqnarray*}
		\mathbf{v}_0 &=& (1, 1, \ldots, 1) \\
		\mathbf{v}_{k+1} &=& (M \widehat{\boxtimes} \mathbf{v}_k) \\
		\mathcal{GE}(M) &=& \lim_{k\to \infty} v_k
	\end{eqnarray*}
	where $M \widehat{\boxtimes} \mathbf{v}$ is a column vector, whose $i^{th}$ entry
	is the geometric average of the vector
	$$\left(v_1 m_{i,1}, v_2 m_{i,2}, \ldots, v_n m_{i,n}\right)$$	
\end{definition}

Our second new priority vector calculation follows the outline of Definition 
\ref{def:eigen}, changing the matrix multiplication, this time doing
a power weighting type calculation.  This leads us to our \ul{Power Weighted Eigenvector} calculation.

\begin{definition}[Power Weighted Eigenvector]
	\label{def:pweigen}
	Let $M$ be an $n \times n$ pairwise comparison matrix.  The 
	\ul{Power Weighted Eigenvector} is the limit of the following sequence of
	vectors $\mathbf{v}_0, \mathbf{v}_1, \ldots$ where
	\begin{eqnarray*}
		\mathbf{v}_0 &=& (1, 1, \ldots, 1) \\
		\mathbf{v}_{k+1} &=& (M ^ {\mathbf{v}_k}) \\
		\mathcal{PWE}(M) &=& \lim_{k\to \infty} v_k
	\end{eqnarray*}
	where $M ^{\mathbf{v}}$ is a column vector, whose $i^{th}$ entry
	is the weighted geometric average $i^{th}$ row of $M$ by $\mathbf{v}$, in
	other words:
	$$\left(M^{\mathbf{v}}\right)_i = \left(\prod_{m_{ij}\neq 0} m_{ij}^{v_j}\right) ^{1/\sum_{m_{ij}\neq 0} v_j} $$
	
\end{definition}

\section{Data/Model Analysis}
In the \texttt{Jupyter} directory of the github repository at \cite{githubahppri}
we have Python based Jupyter workbooks that perform the standard eigenvector 
calculation as well as Definition \ref{def:geigen} and Definition \ref{def:pweigen}.
In the Jupyter workbook \texttt{Comparing\-Eigen\-To\-New\-Calc\-.ipynb}
in \cite{githubahppri} we define the following matrices to perform the new calculations
on.
\begin{eqnarray*}
	&S =
	\begin{pmatrix}
		1 &  1/3 & 3 & 1 \\ 
 		3 & 1 & 1/3  & 3 \\ 
        1/3 & 3 & 1 & 1/9 \\ 
        1 & 1/3 & 9 & 1
	\end{pmatrix}&
	\hspace{0.5in} E =
	\begin{pmatrix}
	  1 &     2 &     3 &     4 \\
  	  1/2 &   1   &  5  &   6 \\
   	  1/3 & 1/5  &  1  &   7  \\
  	  1/4 & 1/6 &  1/7 &  1	
	\end{pmatrix}
	\\
	&M =
	\begin{pmatrix}
		1 &   2 &   0.5 &  1 \\
        0.5 & 1 &   1 &    1 \\
      	2 &   1 &   1 &    1 \\
 		1 &   1 &   1 &    1
	\end{pmatrix} &
	\hspace{0.5in}
	M' = 
	\begin{pmatrix}
        1 & 2 & 1/2 &  1/1.1 \\
        1/2 & 1 & 1 & 1 \\
        2 & 1 & 1 & 1 \\
        1.1 & 1 & 1 & 1		
	\end{pmatrix}
\end{eqnarray*}

The above matrices have the following reason for their inclusion in this paper:
\begin{framed}
\begin{description}
	\item [S:] This was the first user vote that we tried the doppleganger property on, and discovered that the largest eigenvector did not have the that property on this vote set.
	\item [E:] This was a first working example based on the matrix $S$, where we reduced the inconsistency.
	\item [M:] A truly minimal working example showing the largest eigenvector priority vector calculation does not satisfy either the strong or weak doppleganger property.
	\item [M':] A slightly less minimal version of the matrix $M$.
\end{description}	
\end{framed}

\subsection{Results of $S$}
For the matrix $S$ we have the following results.
\begin{eqnarray*}
	\mbox{Inconsistency of } S &=& 0.925 \\
	\mathcal{E}(S) &=& (0.179,  0.297,  0.180,   0.345) \\
	1/\mathcal{E}(S) &=& ( 0.321,  0.193,  0.319,  0.166) \\
	\mathcal{E}(S^T) &=& ( 0.187, 0.240,  0.400,  0.173) \\
	\mathcal{GE}(S) &=& ( 0.238,  0.313,  0.137,  0.313) \\
	1/\mathcal{GE}(S) &=& ( 0.235, 0.179,  0.407,  0.179) \\
	\mathcal{GE}(S^T) &=& ( 0.235, 0.179,  0.407,  0.179) \\
	\mathcal{PWE}(S) &=& (0.212,  0.358,   0.174,  0.256) \\
	1/\mathcal{PWE}(S) &=& (0.275,  0.162,  0.335,  0.227) \\
	\mathcal{PWE}(S^T) &=& (0.232,  0.253,  0.359,  0.156)
\end{eqnarray*}

This example shows:
\begin{enumerate}
	\item the largest eigenvector does not have the either the weak
or strong doppleganger property on this matrix and its transpose.  
	\item the Geometric Eigenvector does have both the strong and weak doppleganger property on this matrix.
	\item the Power Weighted Eigenvector does not have either the strong or weak doppleganger property on this matrix.
\end{enumerate}

\section{Limitations}

\section{Conclusions}


\bibliographystyle{apalike}
\bibliography{OurPapers}

 \section{Appendix 1}
%\begin{center}
%	Just a closing thought:
%\end{center} 
 
%\begin{minipage}[t]{0.5\textwidth}
% \tengwarannataritalic[1.09]
% \tengwa{254}
% \Textendedcalma\TTthreedots\Tnuumen\Tessenuquerna\TTthreedots\Tungwe\Tando\Toore\TTrightcurl\Tumbar\Ttinco\TTthreedots\Tlambealt\TTrightcurl\Tquesse\TTdoublerightcurl
% \Tromanperiod\Ts
% \Textendedcalma\TTthreedots\Tnuumen\Tessenuquerna\TTthreedots\Tungwe\Tungwe\Tumbar\TTnasalizer\TTdot\Ttinco\TTthreedots\Tlambe\TTrightcurl
% \tengwa{255}\\
% \Textendedcalma\TTthreedots\Tnuumen\Tessenuquerna\TTthreedots\Tungwe\Tthuule\Troomen\Tquesse\TTthreedots\Ttinco\TTthreedots\Tlambealt\TTrightcurl\Tquesse\TTdoublerightcurl
% \Tromanperiod\Ts
% \Textendedungwe\TTthreedots\Tumbar\Toore\TTrightcurl\Tesse\Tkern{-0.2}\Tmalta\TTrightcurl\Textendedcalma\TTdot\Ttelco\TTdot\Tquesse\Troomen\Tparma\TTnasalizer\TTdot\Ttinco\TTthreedots\Tlambe\TTrightcurl
% \newline
%\end{minipage}
\begin{minipage}[c]{0.6\textwidth}
\begin{framed}
\includegraphics[width=\textwidth]{2000px-One_Ring_inscription}
\end{framed}
\end{minipage}
\begin{minipage}[c]{0.4\textwidth}
\begin{framed}
 One ring to rule them all \newline
 One ring to find them \newline
 One ring to bring them all \newline
 And in the darkness bind them 
\end{framed}
\end{minipage}

%{\telcontar 
%Ash nazg durbatulûk\newline
%ash nazg gimbatul\newline 
%ash nazg thrakatulûk \newline 
%agh burzum-ishi krimpatul
%}



\end{document}